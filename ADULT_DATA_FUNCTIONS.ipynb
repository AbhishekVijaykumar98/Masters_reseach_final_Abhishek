{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_predef_funcs():\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import pylab\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import sklearn.preprocessing as preprocessing\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import proportion\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_table(train_data,train_labels,test_data,test_labels,\n",
    "#                  model,A,A_,A_str_train,A_str_test,\n",
    "#                  constraint,dr_feat)\n",
    "def create_table(keep_feat,train_data,train_labels,test_data,test_labels,model,A,A_,A_str_train,A_str_test,constraint,dr_feat,dataType):\n",
    "#################################### Imports needed ###################################################\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import pylab\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import sklearn.preprocessing as preprocessing\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import proportion\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "    from fairlearn.reductions import ExponentiatedGradient,EqualizedOdds,DemographicParity\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)\n",
    "\n",
    "################## End of imports & start of actual function code ##################################\n",
    "\n",
    "    abbr_feat = {\"Age\":\"Age\",\"Race\":\"Race\",\"Sex\":\"Sex\",\"Workclass\":\"WrkCls\",\"fnlwgt\":\"fnlwgt\",\"Education-Num\":\"EdNum\",\"Marital Status\":\"MarSts\",\n",
    "                 \"Occupation\":\"Occpn\",\"Relationship\":\"Rltshp\",\"Capital Gain\":\"CapGn\",\"Capital Loss\":\"CapLs\",\n",
    "                 \"Hours per week\":\"HrPWk\",\"Country\":\"Cntry\", \"allFeat\":\"allFeat\"}\n",
    "    \n",
    "    if constraint == 'equalized_odds':\n",
    "        name = \"EO\"\n",
    "        const = EqualizedOdds()\n",
    "    if constraint == 'demographic_parity':\n",
    "        name = \"DP\"\n",
    "        const = DemographicParity()\n",
    "        \n",
    "        \n",
    "    valid_features = [\"Workclass\", \"fnlwgt\", \"Education-Num\", \"Marital Status\",\n",
    "            \"Occupation\", \"Relationship\", \"Capital Gain\", \"Capital Loss\",\n",
    "            \"Hours per week\", \"Country\",\"Age\",\"Race\",\"Sex\"] \n",
    "\n",
    "    rem_feat = [] # need to store all the one hot coded version of the feature to drop\n",
    "    \n",
    "    for things in valid_features:\n",
    "        if things not in keep_feat:\n",
    "            for cols in train_data.columns:\n",
    "                if things in cols:\n",
    "                    rem_feat.append(cols)\n",
    "    if dr_feat is not None:\n",
    "        for cols in train_data.columns:\n",
    "            if dr_feat in cols:\n",
    "                rem_feat.append(cols)\n",
    "    else: \n",
    "        dr_feat = \"allFeat\"\n",
    "    \n",
    "            \n",
    "    # drop the feature we dont want from train and test\n",
    "    train_data = train_data.drop(columns = rem_feat)\n",
    "#     print(train_data.columns)\n",
    "    test_data = test_data.drop(columns = rem_feat)\n",
    "    \n",
    "\n",
    "\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "    lgb_params = {\n",
    "        'objective' : 'binary',\n",
    "        'metric' : 'cross_entropy', \n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves' : 10,\n",
    "        'max_depth' : 3\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(train_data, train_labels)\n",
    "    # Scores on test set\n",
    "    test_scores = model.predict_proba(test_data)[:, 1]\n",
    "    test_pred = model.predict(test_data)\n",
    "    # Train AUC\n",
    "    roc_auc_score(train_labels, model.predict_proba(train_data)[:, 1])\n",
    "    # ACCURACY\n",
    "    accuracy_score(test_labels, test_pred)\n",
    "    # Predictions (0 or 1) on test set\n",
    "    test_preds = (test_scores >= np.mean(train_labels)) * 1\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)\n",
    "    \n",
    "        # Helper functions\n",
    "    def get_metrics_df(models_dict, y_true, group):\n",
    "        metrics_dict = {\n",
    "            \"Overall selection rate\": (\n",
    "                lambda x: selection_rate(y_true, x), True),\n",
    "            \"Demographic parity difference\": (\n",
    "                lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n",
    "            \"False positive rate difference\": (\n",
    "                lambda x: false_positive_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "            \"False negative rate difference\": (\n",
    "                lambda x: false_negative_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "            \"Equalized odds difference\": (\n",
    "                lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n",
    "#             \"Overall AUC\": (\n",
    "#                 lambda x: roc_auc_score(y_true, x), False),\n",
    "            \"Accuracy\": (\n",
    "             lambda x: accuracy_score(y_true, x), True),\n",
    "            \"AUC difference\": (\n",
    "                lambda x: MetricFrame(metrics=roc_auc_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), False),\n",
    "        }\n",
    "        df_dict = {}\n",
    "        for metric_name, (metric_func, use_preds) in metrics_dict.items():\n",
    "            df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n",
    "                                    for model_name, (preds, scores) in models_dict.items()]\n",
    "        return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())\n",
    "    \n",
    "    if dataType == \"threshold_opt\" or dataType ==\"unmitigated\":\n",
    "        postprocess_est = ThresholdOptimizer(\n",
    "            estimator = model,\n",
    "            constraints= constraint,\n",
    "            prefit=True)\n",
    "\n",
    "        balanced_idx1 = train_data[train_labels==1].index\n",
    "        pp_train_idx = balanced_idx1.union(train_labels[train_labels==0].sample(n=balanced_idx1.size, random_state=1234).index)\n",
    "        df_train_balanced = train_data.loc[pp_train_idx, :]\n",
    "        Y_train_balanced = train_labels.loc[pp_train_idx]\n",
    "        A_train_balanced = A_.loc[pp_train_idx]\n",
    "\n",
    "\n",
    "        postprocess_est.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)\n",
    "\n",
    "        postprocess_preds = postprocess_est.predict(test_data, sensitive_features= A)\n",
    "\n",
    "\n",
    "        models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "                      \"ThresholdOptimizer\": (postprocess_preds, postprocess_preds)}\n",
    "        output = get_metrics_df(models_dict, test_labels, A_str_test)\n",
    "        output.rename(columns={'Unmitigated': name+\"_Unmit:\"+abbr_feat[dr_feat], \n",
    "                               'ThresholdOptimizer': name+\"_ThOpt:\"+abbr_feat[dr_feat]}, inplace=True)\n",
    "        \n",
    "    if dataType == \"Exponentiated_grad\":\n",
    "        from fairlearn.reductions import ExponentiatedGradient,EqualizedOdds,DemographicParity\n",
    "#         from xgboost import XGBClassifier\n",
    "#         from sklearn.ensemble import RandomForestClassifier\n",
    "#         clf = RandomForestClassifier(n_estimators=10)\n",
    "#         xgc = XGBClassifier(verbosity=0)\n",
    "\n",
    "        eg = ExponentiatedGradient(estimator= model, constraints= const,max_iter=2)\n",
    "        eg.fit(train_data,train_labels, sensitive_features=A_)\n",
    "        eg_test_preds = eg.predict(test_data)\n",
    "\n",
    "        models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "                      \"ExponentiatedGrad\": (eg_test_preds,eg_test_preds)}\n",
    "        output = (get_metrics_df(models_dict, test_labels, A_str_test))\n",
    "        output.rename(columns={'Unmitigated': name+\"_Unmit:\"+abbr_feat[dr_feat], \n",
    "                       'ExponentiatedGrad': name+\"_ExGrd:\"+abbr_feat[dr_feat]}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Ascore(output,alpha,constraint,dataType = \"unmitigated\"):\n",
    "    \n",
    "#################################### Imports needed ###################################################\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import pylab\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import sklearn.preprocessing as preprocessing\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import proportion\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)\n",
    "\n",
    "################## End of imports & start of actual function code ##################################\n",
    "    if constraint == 'equalized_odds':\n",
    "            name = \"EO\"\n",
    "    else:\n",
    "            name = \"DP\"\n",
    "\n",
    "    if dataType == \"unmitigated\":\n",
    "        col_len = len(output.columns)\n",
    "        del_AUC_unmit = ( output.iloc[5,0] - output.iloc[5,np.arange(0,col_len-1,2)] ) / output.iloc[5,0]\n",
    "        if name == \"EO\":\n",
    "            del_EO_unmit = ( output.iloc[4,0] - output.iloc[4,np.arange(0,col_len-1,2)] ) / output.iloc[4,0]\n",
    "        if name == \"DP\":\n",
    "            del_EO_unmit = ( output.iloc[1,0] - output.iloc[1,np.arange(0,col_len-1,2)] ) / output.iloc[1,0]\n",
    "\n",
    "        del_AUC_unmit = del_AUC_unmit.drop(labels = name+\"_Unmit:allFeat\", axis = 0)\n",
    "        del_EO_unmit = del_EO_unmit.drop(labels = name+\"_Unmit:allFeat\", axis = 0)\n",
    "    \n",
    "    if dataType == \"threshold_opt\":\n",
    "        col_len = len(output.columns)\n",
    "        del_AUC_unmit = ( output.iloc[5,1] - output.iloc[5,np.arange(1,col_len+1,2)] ) / output.iloc[5,1]\n",
    "        if name == \"EO\":\n",
    "            del_EO_unmit = ( output.iloc[4,1] - output.iloc[4,np.arange(1,col_len+1,2)] ) / output.iloc[4,1]\n",
    "        if name == \"DP\":\n",
    "            del_EO_unmit = ( output.iloc[1,1] - output.iloc[1,np.arange(1,col_len+1,2)] ) / output.iloc[1,1]\n",
    "            \n",
    "        del_AUC_unmit = del_AUC_unmit.drop(labels = name+\"_ThOpt:allFeat\", axis = 0)\n",
    "        del_EO_unmit = del_EO_unmit.drop(labels = name+\"_ThOpt:allFeat\", axis = 0)\n",
    "        \n",
    "    if dataType ==\"Exponentiated_grad\":\n",
    "        col_len = len(output.columns)\n",
    "        del_AUC_unmit = ( output.iloc[5,1] - output.iloc[5,np.arange(1,col_len+1,2)] ) / output.iloc[5,1]\n",
    "        if name == \"EO\":\n",
    "            del_EO_unmit = ( output.iloc[4,1] - output.iloc[4,np.arange(1,col_len+1,2)] ) / output.iloc[4,1]\n",
    "        if name == \"DP\":\n",
    "            del_EO_unmit = ( output.iloc[1,1] - output.iloc[1,np.arange(1,col_len+1,2)] ) / output.iloc[1,1]\n",
    "            \n",
    "        del_AUC_unmit = del_AUC_unmit.drop(labels = name+\"_ExGrd:allFeat\", axis = 0)\n",
    "        del_EO_unmit = del_EO_unmit.drop(labels = name+\"_ExGrd:allFeat\", axis = 0)\n",
    "    \n",
    "#     alpha = np.linspace(0,0.5,num=20)\n",
    "\n",
    "    A_score = np.zeros((alpha.shape[0],del_AUC_unmit.shape[0]))\n",
    "    for i in np.arange(0,len(alpha)):\n",
    "        A_score[i,:] =(1.0-alpha[i]) * del_AUC_unmit + alpha[i] * (-del_EO_unmit)\n",
    "     \n",
    "    if dataType ==\"Exponentiated_grad\":\n",
    "        df_col = output.columns.drop([name+\"_Unmit:allFeat\",name+\"_ExGrd:allFeat\"]) \n",
    "    if dataType == \"threshold_opt\":\n",
    "        df_col = output.columns.drop([name+\"_Unmit:allFeat\",name+\"_ThOpt:allFeat\"]) \n",
    "    if dataType == \"unmitigated\":\n",
    "        df_col = output.columns.drop([name+\"_Unmit:allFeat\",name+\"_ThOpt:allFeat\"]) \n",
    "        \n",
    "    A_score_df = pd.DataFrame(A_score, columns = del_AUC_unmit.index ,index=alpha)\n",
    "    \n",
    "    return A_score,A_score_df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
