{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_predef_funcs():\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import pylab\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import sklearn.preprocessing as preprocessing\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import proportion\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_table(train_data,train_labels,test_data,test_labels,\n",
    "#                  model,A,A_,A_str_train,A_str_test,\n",
    "#                  constraint,dr_feat)\n",
    "def create_table(keep_feat,train_data,train_labels,test_data,test_labels,model,A,A_,A_str_train,A_str_test,constraint,dr_feat,dataType):\n",
    "#################################### Imports needed ###################################################\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import pylab\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import sklearn.preprocessing as preprocessing\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import proportion\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds,DemographicParity\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)\n",
    "\n",
    "################## End of imports & start of actual function code ##################################\n",
    "# 'marital_divorced/separated':\"div/sep\", 'marital_divorced/separated/married':\"div/sep/mar\",\n",
    "#        'marital_married/widowed':\"marr/wid\", 'marital_single':\"single\", \n",
    "    abbr_feat = {'duration':\"duration\", 'credit_amount':\"cred_amt\", 'intallment_rate':\"instal_rate\", \n",
    "             'residence_since':\"res_since\",'age':\"age\", 'number_of_existcr':\"num_existcr\", \n",
    "             'number_of_dependents':\"num_dep\", 'telephon':\"telephon\",'marital':\"marital\",\n",
    "       'foreign':\"foreign\", 'sex':\"sex\", 'account_bal_neg_bal':\"acct_neg_bal\", 'account_bal_no_acc':\"acct_no_acct\",\n",
    "       'account_bal_positive_bal':\"acct_pos_bal\", \n",
    "        'payment_status_A30':\"pay_stat_A30\", 'payment_status_A31':\"pay_stat_A31\",\n",
    "       'payment_status_A32':\"pay_stat_A32\", 'payment_status_A33':\"pay_stat_A33\", 'payment_status_A34':\"pay_stat_A34\",\n",
    "       'purpose_A40':\"pur_A40\", 'purpose_A41':\"pur_A41\", 'purpose_A410':\"pur_A410\", 'purpose_A42':\"pur_A42\",\n",
    "       'purpose_A43':\"pur_A43\", 'purpose_A44':\"pur_A44\", 'purpose_A45':\"pur_A45\", 'purpose_A46':\"pur_A46\",\n",
    "       'purpose_A48':\"pur_A48\", 'purpose_A49':\"pur_A49\", 'savings_bond_value_A61':\"sv_bnd_valA61\",\n",
    "       'savings_bond_value_A62':\"sv_bnd_valA62\", 'savings_bond_value_A63':\"sv_bnd_valA63\",\n",
    "       'savings_bond_value_A64':\"sv_bnd_valA64\", 'savings_bond_value_A65':\"sv_bnd_valA65\",\n",
    "       'employed_since_A71':\"empl_A71\", 'employed_since_A72':\"empl_A72\", 'employed_since_A73':\"empl_A73\",\n",
    "       'employed_since_A74':\"empl_A74\", 'employed_since_A75':\"empl_A75\",\n",
    "       'guarantor_A101':\"guar_A101\",'guarantor_A102':\"guar_A102\", 'guarantor_A103':\"guar_A103\", \n",
    "       'most_valuable_asset_car':\"MVA_car\",\n",
    "       'most_valuable_asset_life_insurance':\"MVA_insur\", 'most_valuable_asset_none':\"MVA_none\",\n",
    "       'most_valuable_asset_real_estate':\"MVA_real_est\", \n",
    "       'concurrent_credits_A141':\"concu_cred_A141\",'concurrent_credits_A142':\"concu_cred_A142\", \n",
    "       'concurrent_credits_A143':\"concu_cred_A143\",\n",
    "       'type_of_housing_A151':\"house_A151\", 'type_of_housing_A152':\"house_A152\", 'type_of_housing_A153':\"house_A153\",\n",
    "       'job_highly_skilled':\"highly_skilled\", 'job_skilled':\"skilled\", 'job_unskilled':\"unskilled\", \"allFeat\":\"allFeat\"}\n",
    "    \n",
    "    if constraint == 'equalized_odds':\n",
    "        name = \"EO\"\n",
    "        const = EqualizedOdds()\n",
    "    if constraint == 'demographic_parity':\n",
    "        name = \"DP\"\n",
    "        const = DemographicParity()\n",
    "        \n",
    "        \n",
    "    valid_features = ['duration', 'credit_amount', 'intallment_rate', 'marital',\n",
    "       'residence_since', 'age', 'number_of_existcr', 'number_of_dependents',\n",
    "       'telephon', 'foreign', 'sex', 'account_bal_neg_bal',\n",
    "       'account_bal_no_acc', 'account_bal_positive_bal', 'payment_status_A30',\n",
    "       'payment_status_A31', 'payment_status_A32', 'payment_status_A33',\n",
    "       'payment_status_A34', 'purpose_A40', 'purpose_A41', 'purpose_A410',\n",
    "       'purpose_A42', 'purpose_A43', 'purpose_A44', 'purpose_A45',\n",
    "       'purpose_A46', 'purpose_A48', 'purpose_A49', 'savings_bond_value_A61',\n",
    "       'savings_bond_value_A62', 'savings_bond_value_A63',\n",
    "       'savings_bond_value_A64', 'savings_bond_value_A65',\n",
    "       'employed_since_A71', 'employed_since_A72', 'employed_since_A73',\n",
    "       'employed_since_A74', 'employed_since_A75', 'guarantor_A101',\n",
    "       'guarantor_A102', 'guarantor_A103', 'most_valuable_asset_car',\n",
    "       'most_valuable_asset_life_insurance', 'most_valuable_asset_none',\n",
    "       'most_valuable_asset_real_estate', 'concurrent_credits_A141',\n",
    "       'concurrent_credits_A142', 'concurrent_credits_A143',\n",
    "       'type_of_housing_A151', 'type_of_housing_A152', 'type_of_housing_A153',\n",
    "       'job_highly_skilled', 'job_skilled', 'job_unskilled']\n",
    "\n",
    "    rem_feat = [] # need to store all the one hot coded version of the feature to drop\n",
    "    \n",
    "    for things in valid_features:\n",
    "        if things not in keep_feat:\n",
    "            for cols in train_data.columns:\n",
    "                if things in cols:\n",
    "                    rem_feat.append(cols)\n",
    "    if dr_feat is not None:\n",
    "        for cols in train_data.columns:\n",
    "            if dr_feat in cols:\n",
    "                rem_feat.append(cols)\n",
    "    else: \n",
    "        dr_feat = \"allFeat\"\n",
    "    \n",
    "            \n",
    "    # drop the feature we dont want from train and test\n",
    "    train_data = train_data.drop(columns = rem_feat)\n",
    "#     print(train_data.columns)\n",
    "    test_data = test_data.drop(columns = rem_feat)\n",
    "    \n",
    "\n",
    "\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "    lgb_params = {\n",
    "        'objective' : 'binary',\n",
    "        'metric' : 'cross_entropy', \n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves' : 10,\n",
    "        'max_depth' : 3\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(train_data, train_labels)\n",
    "    # Scores on test set\n",
    "    test_scores = model.predict_proba(test_data)[:, 1]\n",
    "    test_pred = model.predict(test_data)\n",
    "    # Train AUC\n",
    "    roc_auc_score(train_labels, model.predict_proba(train_data)[:, 1])\n",
    "    # ACCURACY\n",
    "    accuracy_score(test_labels, test_pred)\n",
    "    # Predictions (0 or 1) on test set\n",
    "    test_preds = (test_scores >= np.mean(train_labels)) * 1\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)\n",
    "    \n",
    "        # Helper functions\n",
    "    def get_metrics_df(models_dict, y_true, group):\n",
    "        metrics_dict = {\n",
    "            \"Overall selection rate\": (\n",
    "                lambda x: selection_rate(y_true, x), True),\n",
    "            \"Demographic parity difference\": (\n",
    "                lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n",
    "            \"False positive rate difference\": (\n",
    "                lambda x: false_positive_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "            \"False negative rate difference\": (\n",
    "                lambda x: false_negative_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "            \"Equalized odds difference\": (\n",
    "                lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n",
    "#             \"Overall AUC\": (\n",
    "#                 lambda x: roc_auc_score(y_true, x), False),\n",
    "            \"Accuracy\": (\n",
    "             lambda x: accuracy_score(y_true, x), True),\n",
    "            \"AUC difference\": (\n",
    "                lambda x: MetricFrame(metrics=roc_auc_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), False),\n",
    "        }\n",
    "        df_dict = {}\n",
    "        for metric_name, (metric_func, use_preds) in metrics_dict.items():\n",
    "            df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n",
    "                                    for model_name, (preds, scores) in models_dict.items()]\n",
    "        return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())\n",
    "    \n",
    "    if dataType == \"threshold_opt\" or dataType ==\"unmitigated\":\n",
    "        postprocess_est = ThresholdOptimizer(\n",
    "            estimator = model,\n",
    "            constraints= constraint,\n",
    "            prefit=True)\n",
    "\n",
    "        balanced_idx1 = train_data[train_labels==1].index\n",
    "        pp_train_idx = balanced_idx1.union(train_labels[train_labels==0].sample(n=balanced_idx1.size, random_state=1234).index)\n",
    "        df_train_balanced = train_data.loc[pp_train_idx, :]\n",
    "        Y_train_balanced = train_labels.loc[pp_train_idx]\n",
    "        A_train_balanced = A_.loc[pp_train_idx]\n",
    "\n",
    "\n",
    "        postprocess_est.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)\n",
    "\n",
    "        postprocess_preds = postprocess_est.predict(test_data, sensitive_features= A)\n",
    "\n",
    "\n",
    "        models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "                      \"ThresholdOptimizer\": (postprocess_preds, postprocess_preds)}\n",
    "        output = get_metrics_df(models_dict, test_labels, A_str_test)\n",
    "        output.rename(columns={'Unmitigated': name+\"_Unmit:\"+abbr_feat[dr_feat], \n",
    "                               'ThresholdOptimizer': name+\"_ThOpt:\"+abbr_feat[dr_feat]}, inplace=True)\n",
    "        \n",
    "    if dataType == \"Exponentiated_grad\":\n",
    "        from fairlearn.reductions import ExponentiatedGradient,EqualizedOdds,DemographicParity\n",
    "#         from xgboost import XGBClassifier\n",
    "#         from sklearn.ensemble import RandomForestClassifier\n",
    "#         clf = RandomForestClassifier(n_estimators=10)\n",
    "#         xgc = XGBClassifier(verbosity=0)\n",
    "\n",
    "        eg = ExponentiatedGradient(estimator= model, constraints= const,max_iter=2)\n",
    "        eg.fit(train_data,train_labels, sensitive_features=A_)\n",
    "        eg_test_preds = eg.predict(test_data)\n",
    "\n",
    "        models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "                      \"ExponentiatedGrad\": (eg_test_preds,eg_test_preds)}\n",
    "        output = (get_metrics_df(models_dict, test_labels, A_str_test))\n",
    "        output.rename(columns={'Unmitigated': name+\"_Unmit:\"+abbr_feat[dr_feat], \n",
    "                       'ExponentiatedGrad': name+\"_ExGrd:\"+abbr_feat[dr_feat]}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Ascore(output,alpha,constraint,dataType = \"unmitigated\"):\n",
    "    \n",
    "#################################### Imports needed ###################################################\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import pylab\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn import linear_model\n",
    "    import sklearn.preprocessing as preprocessing\n",
    "    import sklearn.metrics as metrics\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import proportion\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score, roc_auc_score,accuracy_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Fairlearn algorithms and utils\n",
    "    from fairlearn.postprocessing import ThresholdOptimizer\n",
    "    from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "    # Metrics\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "        false_positive_rate, false_negative_rate,\n",
    "        false_positive_rate_difference, false_negative_rate_difference,\n",
    "        equalized_odds_difference, count)\n",
    "\n",
    "################## End of imports & start of actual function code ##################################\n",
    "    if constraint == 'equalized_odds':\n",
    "            name = \"EO\"\n",
    "    else:\n",
    "            name = \"DP\"\n",
    "\n",
    "    if dataType == \"unmitigated\":\n",
    "        col_len = len(output.columns)\n",
    "        del_AUC_unmit = ( output.iloc[5,0] - output.iloc[5,np.arange(0,col_len-1,2)] ) / output.iloc[5,0]\n",
    "        if name == \"EO\":\n",
    "            del_EO_unmit = ( output.iloc[4,0] - output.iloc[4,np.arange(0,col_len-1,2)] ) / output.iloc[4,0]\n",
    "        if name == \"DP\":\n",
    "            del_EO_unmit = ( output.iloc[1,0] - output.iloc[1,np.arange(0,col_len-1,2)] ) / output.iloc[1,0]\n",
    "\n",
    "        del_AUC_unmit = del_AUC_unmit.drop(labels = name+\"_Unmit:allFeat\", axis = 0)\n",
    "        del_EO_unmit = del_EO_unmit.drop(labels = name+\"_Unmit:allFeat\", axis = 0)\n",
    "    \n",
    "    if dataType == \"threshold_opt\":\n",
    "        col_len = len(output.columns)\n",
    "        del_AUC_unmit = ( output.iloc[5,1] - output.iloc[5,np.arange(1,col_len+1,2)] ) / output.iloc[5,1]\n",
    "        if name == \"EO\":\n",
    "            del_EO_unmit = ( output.iloc[4,1] - output.iloc[4,np.arange(1,col_len+1,2)] ) / output.iloc[4,1]\n",
    "        if name == \"DP\":\n",
    "            del_EO_unmit = ( output.iloc[1,1] - output.iloc[1,np.arange(1,col_len+1,2)] ) / output.iloc[1,1]\n",
    "            \n",
    "        del_AUC_unmit = del_AUC_unmit.drop(labels = name+\"_ThOpt:allFeat\", axis = 0)\n",
    "        del_EO_unmit = del_EO_unmit.drop(labels = name+\"_ThOpt:allFeat\", axis = 0)\n",
    "        \n",
    "    if dataType ==\"Exponentiated_grad\":\n",
    "        col_len = len(output.columns)\n",
    "        del_AUC_unmit = ( output.iloc[5,1] - output.iloc[5,np.arange(1,col_len+1,2)] ) / output.iloc[5,1]\n",
    "        if name == \"EO\":\n",
    "            del_EO_unmit = ( output.iloc[4,1] - output.iloc[4,np.arange(1,col_len+1,2)] ) / output.iloc[4,1]\n",
    "        if name == \"DP\":\n",
    "            del_EO_unmit = ( output.iloc[1,1] - output.iloc[1,np.arange(1,col_len+1,2)] ) / output.iloc[1,1]\n",
    "            \n",
    "        del_AUC_unmit = del_AUC_unmit.drop(labels = name+\"_ExGrd:allFeat\", axis = 0)\n",
    "        del_EO_unmit = del_EO_unmit.drop(labels = name+\"_ExGrd:allFeat\", axis = 0)\n",
    "    \n",
    "#     alpha = np.linspace(0,0.5,num=20)\n",
    "\n",
    "    A_score = np.zeros((alpha.shape[0],del_AUC_unmit.shape[0]))\n",
    "    for i in np.arange(0,len(alpha)):\n",
    "        A_score[i,:] =(1.0-alpha[i]) * del_AUC_unmit + alpha[i] * (-del_EO_unmit)\n",
    "     \n",
    "    if dataType ==\"Exponentiated_grad\":\n",
    "        df_col = output.columns.drop([name+\"_Unmit:allFeat\",name+\"_ExGrd:allFeat\"]) \n",
    "    if dataType == \"threshold_opt\":\n",
    "        df_col = output.columns.drop([name+\"_Unmit:allFeat\",name+\"_ThOpt:allFeat\"]) \n",
    "    if dataType == \"unmitigated\":\n",
    "        df_col = output.columns.drop([name+\"_Unmit:allFeat\",name+\"_ThOpt:allFeat\"]) \n",
    "        \n",
    "    A_score_df = pd.DataFrame(A_score, columns = del_AUC_unmit.index ,index=alpha)\n",
    "    \n",
    "    return A_score,A_score_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
